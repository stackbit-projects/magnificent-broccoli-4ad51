---
title: The Manifesto
subtitle: >-
  At Einst.AI, We're embracing the meta universe and consider ourselves a
  meta-learning enterprise that is model-agnostic, in the sense that we're
  pretty compatible with any model trained with stochastic gradient descent and
  applicable to a variety of different learning problems, including
  classification, regression, and reinforcement learning.
image: /images/Transparent.png
image_alt: Team members in a conference room
seo:
  title: About Us
  description: This is the about page
  extra:
    - name: 'og:type'
      value: website
      keyName: property
    - name: 'og:title'
      value: About Us
      keyName: property
    - name: 'og:description'
      value: This is the about page
      keyName: property
    - name: 'og:image'
      value: images/about.jpg
      keyName: property
      relativeUrl: true
    - name: 'twitter:card'
      value: summary_large_image
    - name: 'twitter:title'
      value: About Us
    - name: 'twitter:description'
      value: This is the about page
    - name: 'twitter:image'
      value: images/about.jpg
      relativeUrl: true
layout: AdvancedLayout
sections:
  - colors: colors-a
    width: full
    height: short
    topGap: none
    bottomGap: none
    alignHoriz: center
    text: "Einst.AI uses\_\\*domain randomization\_\\*to train in a very diverse set of simulated environments that enables transfer to the real world. Domain randomization is the extension of data augmentation, which has been used in computer vision since the inception of convolutional networks, from data sets to simulators. Randomizing many aspects of the simulation that do not match the real world forces the learned model to be robust to these variations.\n\n*Distractions*\_can look technically similar to domain randomization but distractions are part of the problem that the agent has to solve rather than part of the solution. As a result, the agent does not have control over distractions, i.e. cannot affect these distractions, cannot arbitrarily sample more of them, and has to handle them during evaluation.\n\nThe magnitude of each distraction type can be controlled by a “difficulty magnitude” scalar between 0 and 1. Distractions can be set to either change during episodes or change only between episodes, which we will refer to as\_\\*dynamic\_\\*and\_\\*static\_\\*settings, respectively.\n\n# Transformer Models with IMAGINATION\n\nDifferent from such a database-dependent tree encoding, our zero-shot encoding uses a filtered stateless hash-tree-based encoding where features of nodes are database-independent (i.e., they can be derived from any database at hand). As a result, queries over different databases can be expressed as input to einst.ai. Domain randomization is the extension of data augmentation, which has been used in computer vision since the inception of convolutional networks \\[17], from data sets to simulators. Randomizing many aspects of the simulation that do not match the real world forces the learned model to be robust to these variations.\n\n*Distractions*, which this paper focuses on, can look techni- cally similar to domain randomization but distractions as we define them here are part of the problem that the agent has to solve rather than part of the solution. As a result, the agent does not have control over distractions, i.e. cannot affect these distractions, cannot arbitrarily sample more of them, and has to handle them during evaluation.\n\n### IMAGIN8\n\nOne crucial aspect of zero-shot models is to decide what should be learned by the models to fulfill its core promise and when to separate concerns. For example, workload-driven approaches often prefer end-to-end learning which captures both data-and system-characteristics in a single model. However, this results in models that do not generalize to new databases with different data characteristics. Domain randomization is the extension of data augmentation, which has been used in computer vision since the inception of convolutional networks \\[17], from data sets to simulators. Randomizing many aspects of the simulation that do not match the real world forces the learned model to be robust to these variations.\n\n*Distractions*, which this paper focuses on, can look techni- cally similar to domain randomization but distractions as we define them here are part of the problem that the agent has to solve rather than part of the solution. As a result, the agent does not have control over distractions, i.e. cannot affect these distractions, cannot arbitrarily sample more of them, and has to handle them during evaluation.\n\n## Model Agnostic VIN\n\nOur main contribution came about with DEJA, a DARPA funded project which we participaterd in and achiebed notoriety. DEJA is built with Unity3D it comprises a MMORPG that requires users to engage in deep meditation, it even forces individuals to leave the phone on a flat surface while the meditation takes place and is measured as such. Our first experiment domain is a synthetic grid-world with randomly placed obstacles, in which the observation includes the position of the agent, and also an image of the map of obstacles and goal position. Figure 3 shows two random instances of such a grid-world of size\_16\_×\_16. We conjecture that by learning the optimal policy for several instances of this domain, a VIN policy would learn the planning computation required to solve a new, unseen, task.\n\n## Practical Distributed Deep Neural Networks and Transformers-as-a-Service\n\nWe deployed an einst.ai instance along with a paralley run database instance via EinsteinDB's MilevaDB and FIDel with BerolinaSQL oversought by NEO and LUX-MARINA. The grid powered by LMDB (BerkeleyDB) and MilevaDB, with a size of 49GB on single node server with 4 cores, 32GB of RAM generated 1,\_000 random query instances out of these 99 templates and placed them in a random order in the execution queue. The benchmark includes 165 tables and indexes, and the number of blocks for each of these ranged between 100 and 130,\_0000. However, after downsizing both the query vector and buffer state bitmaps, our representation vectors have a size of 165×1,000, including index tables. We run our experiments on BerolinaSQL with a shared buffer pool size of 2GB.1\_For each query, we collect its query plan without executing the query by using the\_EXPLAIN\_command.\n\n> \"Imagination is More Important Than Knowledge.” - Albert Einstein\n\n#### Service Level Agreements (SLAs) as Policy: Real World Applications\n\nWe assume that there is an underlying\_\\*state\_\\*s\_(e.g. xy-locations of objects in a game), whose features may be very well structured, but that this state has been projected to a high dimensional observation space by\_Improving raw workload latency, Integrating query priorities and customizable Service Level Agreements (SLAs) into einst.ai by modifying the reward signal could result in a buffer-aware and SLA- compliant scheduler.\n\nTo improve the latency for skew queries, einst.ai augments the training data with access frequencies. To address the slow model re-training when data distribution shifts, einst.ai caches the previously-trained models and incrementally fine-tunes them for similar access patterns and data distribution. EinstAI improves the query latency by 45.1% and reduces the model re-training time to 1/20.\n\nWorkload-aware performance tuning is widely used in cloud databases like Amazon Redshift. Like Redshift, einst.ai trains prediction models on each cluster so that the decisions are optimal for the workload served by that cluster. To have a better prediction quality and account for drift, the underlying models are refreshed periodically. The models predict the query execution time and memory usage for each query in the workload. With this knowledge, the workload manager, einst.ai, makes scheduling and resource allocation decisions wisely. For example, einst.ai assigns the right amount of memory to queries to fully utilize resources and schedules short-running queries before longer queries in terms of the execution time. In addition, when long running queries with large amount of resource usage block incoming short queries, EinstAI will preempt the long running queries to make room for short queries. Furthermore, when the workload is heavy and the current cluster resources are fully utilized, einst.aiwill automatically scale queries out to new clusters.\n"
    elementId: lorem-ipsum
    type: ContentSection
    title: The Manifesto
  - colors: colors-a
    width: wide
    height: short
    topGap: small
    bottomGap: small
    alignHoriz: left
    badge: lorem-ipsum
    title: lorem-ipsum
    subtitle: lorem-ipsum
    text: >-
      ## Lorem ipsum


      Lorem ipsum dolor sit amet, **consectetur adipiscing elit**, sed do
      eiusmod tempor incididunt ut labore et dolore magna aliqua.


      - Lorem ipsum

      - dolor sit amet
    elementId: lorem-ipsum
    type: ContentSection
---
